{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benchpress Hackathon\n",
    "\n",
    "The challenge comes with a Jupyter notebook for your implementation and various utilities.\n",
    "We provide a development set and a validation set you can use to develop your solution.\n",
    "The development set is for testing your code and consists of 300 problems with a varying number of test cases.\n",
    "You are free to use all data provided with a problem, a sample has the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    # Unique identifier for the problem in the APPS dataset.\n",
    "    \"problem_id\": 4424,\n",
    "    # The problem statement\n",
    "    \"question\": \"Given three integers ...\",\n",
    "    # The expected function name and the input/output examples\n",
    "    # representing test cases.\n",
    "    \"input_output\": {\n",
    "        \"fn_name\": \"expression_matter\",\n",
    "        \"inputs\": [ ... ],\n",
    "        \"outputs\": [ ... ]\n",
    "    },\n",
    "    \"url\": \"https://www.codewars.com/kata/5ae62fcf252e66d44d00008e\",\n",
    "    \"difficulty\": \"introductory\",\n",
    "    # The starter code for the problem.\n",
    "    \"starter_code\": \"def expression_matter(a, b, c):\\n\\t\"\n",
    "}\n",
    "```\n",
    "\n",
    "The validation set is consists of 200 problems, and includes an additional key `test_cases` which is used to score your solution with the provided scoring function.\n",
    "\n",
    "```python\n",
    "{\n",
    "    ...\n",
    "    \"test_cases\": {\n",
    "        \"fn_name\": \"expression_matter\",\n",
    "        \"inputs\": [ ... ],\n",
    "        \"outputs\": [ ... ]\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "### Loading Problems\n",
    "\n",
    "Use the `load_sample` function to load a problem from the development or validation set.\n",
    "\n",
    "```python\n",
    "from utilities import load_sample\n",
    "\n",
    "problem = load_sample(index=0, dataset_path=\"./data/dev\")\n",
    "```\n",
    "\n",
    "### Generating Code\n",
    "\n",
    "Use the `aleph_alpha_client` to generate code.\n",
    "Make sure your `AA_TOKEN` is set.\n",
    "\n",
    "```python\n",
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "\n",
    "client = Client(AA_TOKEN)\n",
    "\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(\"Your prompt.\"),\n",
    "    maximum_tokens=256,\n",
    ")\n",
    "\n",
    "# API reference for the client:\n",
    "# https://aleph-alpha-client.readthedocs.io/en/latest/\n",
    "response = client.complete(request, model=MODEL)\n",
    "```\n",
    "\n",
    "### Running Tests\n",
    "\n",
    "Use the `run_test_cases` function to run the generated code against the test cases.\n",
    "The function returns a dictionary with the test results, including the expected output, the generated output, a boolean indicating whether the test passed and a traceback in case of an error.\n",
    "\n",
    "```python\n",
    "from utilities import run_test_cases\n",
    "\n",
    "test_results = run_test_cases(\n",
    "    problem=problem, \n",
    "    generation=response.completions[0].completion, \n",
    "    timeout=10,\n",
    ")\n",
    "```\n",
    "\n",
    "### Scoring\n",
    "\n",
    "Use the `score` function to score your solution on the validation set.\n",
    "It expects a function that takes a problem and a client and returns a generation.\n",
    "\n",
    "```python\n",
    "from utilities import score\n",
    "\n",
    "passed_problems, passed_test_cases = score(\n",
    "    generation_func=generate_code, \n",
    "    client=client,\n",
    "    dataset_path=\"./data/val\", \n",
    "    length=50,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T08:23:09.360039Z",
     "start_time": "2024-11-23T08:23:09.356398Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AA_TOKEN = \"\"\n",
    "# MODEL = \"llama-3.1-8b-instruct-long-context\"\n",
    "MODEL = \"llama-3.1-70b-instruct-long-context\"\n",
    "\n",
    "if AA_TOKEN is None:\n",
    "    raise ValueError(\"Aleph Alpha Playground token is not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n",
      "def expression_matter(a, b, c):\n",
      "    return max(a * (b + c), a + b * c, (a + b) * c, a + b + c)\n"
     ]
    }
   ],
   "source": [
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "from utilities import load_sample, run_test_cases\n",
    "\n",
    "\n",
    "client = Client(AA_TOKEN)\n",
    "\n",
    "\n",
    "def generate_prompt(problem: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for a given problem.\n",
    "    \n",
    "    Args:\n",
    "        problem (dict): A dictionary containing the problem, test cases, and starter code.\n",
    "    \n",
    "    Returns:\n",
    "        str: A prompt for the given problem.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\\nQUESTION:\\n\"\n",
    "    prompt += problem[\"question\"]\n",
    "    prompt += \"\\n\\nSTARTER CODE:\\n\"\n",
    "    prompt += problem[\"starter_code\"]\n",
    "    prompt += \"\\n\\nWrite only the code, directly executable, no tests or other comments.\"\n",
    "    prompt += \" Don't use markdown code blocks.\"\n",
    "    prompt += \"\\n\\nCODE:\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_code(problem: dict, client: Client) -> str:\n",
    "    \"\"\"\n",
    "    Implement the generation function.\n",
    "    \n",
    "    Args:\n",
    "        problem (dict): The problem to solve.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated code.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = generate_prompt(problem=problem)\n",
    "\n",
    "    request = CompletionRequest(\n",
    "        prompt=Prompt.from_text(prompt),\n",
    "        maximum_tokens=64,\n",
    "    )\n",
    "\n",
    "    # API reference for the client:\n",
    "    # https://aleph-alpha-client.readthedocs.io/en/latest/\n",
    "    response = client.complete(request, model=MODEL)\n",
    "    \n",
    "    # Run test cases on the generated code and iterate.\n",
    "    # test_results = run_test_cases(\n",
    "    #    problem=problem, \n",
    "    #    generation=response.completions[0].completion, \n",
    "    #    timeout=10,\n",
    "    # )\n",
    "\n",
    "    return response.completions[0].completion\n",
    "\n",
    "\n",
    "problem = load_sample(index=0, dataset_path=\"./data/dev\")\n",
    "generated_code = generate_code(problem, client)\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:05,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type 0 compilation error = invalid syntax (<string>, line 22)\n",
      "[{'passed': False, 'input': None, 'output': None, 'expected_output': None, 'traceback': 'Traceback (most recent call last):\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/utilities/testing_util.py\", line 185, in run_test\\n    tmp_sol = RuntimeModule.from_string(\"tmp_sol\", \"\", sol)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 169, in _newf\\n    return self._items[f.__name__][len(args)](*args, **kwargs)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 279, in from_string\\n    _exec(s, g)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 97, in _exec\\n    def _exec(m,g): exec(m,g)\\n  File \"<string>\", line 22\\n    def binomial_coeff\\n                      ^\\nSyntaxError: invalid syntax\\n'}]\n",
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:02<00:04,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'passed': False, 'input': [[0.5, 0.5, 0.5], 30], 'output': [None], 'expected_output': [[0.5, 0.5, 0.5, 1.5, 2.5, 4.5, 8.5, 15.5, 28.5, 52.5, 96.5, 177.5, 326.5, 600.5, 1104.5, 2031.5, 3736.5, 6872.5, 12640.5, 23249.5, 42762.5, 78652.5, 144664.5, 266079.5, 489396.5, 900140.5, 1655616.5, 3045153.5, 5600910.5, 10301680.5]], 'traceback': None}]\n",
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:04<00:02,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type 0 compilation error = invalid syntax (<string>, line 25)\n",
      "[{'passed': False, 'input': None, 'output': None, 'expected_output': None, 'traceback': 'Traceback (most recent call last):\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/utilities/testing_util.py\", line 185, in run_test\\n    tmp_sol = RuntimeModule.from_string(\"tmp_sol\", \"\", sol)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 169, in _newf\\n    return self._items[f.__name__][len(args)](*args, **kwargs)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 279, in from_string\\n    _exec(s, g)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 97, in _exec\\n    def _exec(m,g): exec(m,g)\\n  File \"<string>\", line 25\\n    for\\n       ^\\nSyntaxError: invalid syntax\\n'}]\n",
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:05<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type 0 compilation error = '[' was never closed (<string>, line 25)\n",
      "[{'passed': False, 'input': None, 'output': None, 'expected_output': None, 'traceback': 'Traceback (most recent call last):\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/utilities/testing_util.py\", line 185, in run_test\\n    tmp_sol = RuntimeModule.from_string(\"tmp_sol\", \"\", sol)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 169, in _newf\\n    return self._items[f.__name__][len(args)](*args, **kwargs)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 279, in from_string\\n    _exec(s, g)\\n  File \"/Users/gregor.ziegltrum/PycharmProjects/benchpress-hackathon/.venv/lib/python3.10/site-packages/pyext.py\", line 97, in _exec\\n    def _exec(m,g): exec(m,g)\\n  File \"<string>\", line 25\\n    return max([item\\n               ^\\nSyntaxError: \\'[\\' was never closed\\n'}]\n",
      "['problem_id', 'question', 'input_output', 'url', 'difficulty', 'starter_code']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'passed': False, 'input': ['PLPPLPLLEELELRPFFMAAGGTPLAMMGG'], 'output': [None], 'expected_output': [50], 'traceback': None}]\n",
      "Passed 0.0% of problems\n",
      "Passed 0.0% of test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utilities import score\n",
    "\n",
    "passed_problems, passed_test_cases = score(\n",
    "    generation_func=generate_code, \n",
    "    client=client,\n",
    "    dataset_path=\"./data/val\", \n",
    "    length=5,\n",
    ")\n",
    "\n",
    "print(f\"Passed {passed_problems*100}% of problems\")\n",
    "print(f\"Passed {passed_test_cases*100}% of test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
